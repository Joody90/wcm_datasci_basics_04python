{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "e2e917defab390d1af137531e686ba57dfb219263c26fe0a3bd8b1f36a5e7d1f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Assignment 4 Part 2 - Effect size, power, and p-values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "First import the libraries you will use, notice we will be calling `numpy` with `np.` notation, and `matplotlib.pyplot` as `plt`. These are conventional ways to call these libraries:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "source": [
    "## Practice with synthetic data\n",
    "\n",
    "First, it is important to know that most random number generators are pseudo-random. Meaning, the random numbers are generated with a seed or initial value, and if you declare this seed at the beginning of your analysis, your randomly generated numbers will be consistent every time your code is ran, giving you reproducible results. In `numpy`'s `random` module, you can declare your random number generator with `seed`, see this answer here for an example: https://stackoverflow.com/a/21494630\n",
    "\n",
    "There are numerous ways to generate random data, you can check out all the `numpy.random` capabilities here: https://numpy.org/doc/1.16/reference/routines.random.html\n",
    "\n",
    "For now, we will start with the standard normally distributed data (Gaussians). Start by generating 20 random data points with `numpy`'s `randn`, and visualizing it as a histogram with `matplotlib`'s `hist`: https://matplotlib.org/3.3.4/api/_as_gen/matplotlib.pyplot.hist.html"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random number generator initial seed/value:\n",
    "\n",
    "\n",
    "# Create a variable that has 20 normally distributed data points. (Use numpy.random.randn)\n",
    "data = \n",
    "\n",
    "# Visualize the data you just created with `plt.hist()`. You can tell the histogram to display bin sizes to your liking with `plt.hist(data, bins=#)`. Play around with bin sizes until a decent looking histogram appears.\n",
    "figure1, axes1 = plt.subplots()     # initialize a figure and axes for drawing\n",
    "axes1.hist(data, bins = 5)          # Input your data and bins\n",
    "axes1.set_title('Histogram of Synthetic Data')"
   ]
  },
  {
   "source": [
    "A normal (Gaussian) distribution is sufficiently defined by 2 statistics, it's mean and variance. `numpy` has a `mean()` and a `std()` function to compute the mean and standard deviation of your data. Compute the mean and standard deviation of your dataset and use the `print()` function to display the values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean: \n",
    "data_mean = \n",
    "\n",
    "# compute the variance: \n",
    "data_std = \n",
    "\n",
    "print('Data mean is ', data_mean)\n",
    "print('Data standard deviation is ', data_std)"
   ]
  },
  {
   "source": [
    "If you carefully read `randn`'s documentation, you should have known that `randn` samples from a \"standard\" normal distribution, which as a mean = 0 and standard deviation of 1. Does your `data`'s mean and standard deviation match this expectation? If not, why not?\n",
    "\n",
    " > Answer in this markdown quotation block (`>`) here"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Now instead of 20 data points, replace `data` with 1000 data points from `randn`, display the histogram and compute the mean and variances again:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed your Random Number Generator (RNG)\n",
    "\n",
    "# Replace data with 1000 points:\n",
    "data = \n",
    "\n",
    "# compute and mean and variances, print the answer out like before:\n",
    "new_mean = \n",
    "new_std = \n",
    "\n",
    "# display the histogram with matplotlib\n"
   ]
  },
  {
   "source": [
    "## A little more realistic scenario.\n",
    "\n",
    "Now, suppose you are recording the firing rate of neurons pre-treatment and post-treatment. Let's say your treatment is hypothesized to increase the firing rate of neurons. \n",
    "\n",
    "From your pilot study, you recorded from a small field of view that has 10 neurons, and your recording software summarized the recording and told you:\n",
    "\n",
    " - N = 10 neurons \n",
    " - Pre-treatment data firing rate per second: mean = 150\n",
    " - Post-treatment data firing rate per second: mean = 160\n",
    " - Both datasets have about the same standard deviation $\\sigma^2=30$, and assume normal distribution.\n",
    "\n",
    "Use `np.random.normal()` to generate your 2 datasets, see docs here: https://numpy.org/doc/1.16/reference/generated/numpy.random.normal.html#numpy.random.normal\n",
    "\n",
    " - `np.random.normal` draws samples from a normal distribution, the first input `loc` is the mean of the distribution, the second input `scale` is the standard deviation, and the last is the amount of samples you want to draw. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-62-caa271106a47>, line 5)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-62-caa271106a47>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    pre_data  =\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Generate 2 fake datasets according to the pilot study scenario above, do not use a `seed` this time, also no need to plot the histogram:\n",
    "N  = 10\n",
    "s2 = 30     # standard deviation is the same for both groups\n",
    "\n",
    "pre_data  =\n",
    "post_data ="
   ]
  },
  {
   "source": [
    "After seeing this data, you got very excited and wanted to see if you got your hands on a significant finding, Perform a 2-sampled t-test with `scipy.stats.ttest_ind`, look at the documentations with `?` and plug in your data: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mttest_ind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mequal_var\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnan_policy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'propagate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Calculate the T-test for the means of *two independent* samples of scores.\n",
      "\n",
      "This is a two-sided test for the null hypothesis that 2 independent samples\n",
      "have identical average (expected) values. This test assumes that the\n",
      "populations have identical variances by default.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "a, b : array_like\n",
      "    The arrays must have the same shape, except in the dimension\n",
      "    corresponding to `axis` (the first, by default).\n",
      "axis : int or None, optional\n",
      "    Axis along which to compute test. If None, compute over the whole\n",
      "    arrays, `a`, and `b`.\n",
      "equal_var : bool, optional\n",
      "    If True (default), perform a standard independent 2 sample test\n",
      "    that assumes equal population variances [1]_.\n",
      "    If False, perform Welch's t-test, which does not assume equal\n",
      "    population variance [2]_.\n",
      "\n",
      "    .. versionadded:: 0.11.0\n",
      "nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "    Defines how to handle when input contains nan.\n",
      "    The following options are available (default is 'propagate'):\n",
      "\n",
      "      * 'propagate': returns nan\n",
      "      * 'raise': throws an error\n",
      "      * 'omit': performs the calculations ignoring nan values\n",
      "\n",
      "Returns\n",
      "-------\n",
      "statistic : float or array\n",
      "    The calculated t-statistic.\n",
      "pvalue : float or array\n",
      "    The two-tailed p-value.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "We can use this test, if we observe two independent samples from\n",
      "the same or different population, e.g. exam scores of boys and\n",
      "girls or of two ethnic groups. The test measures whether the\n",
      "average (expected) value differs significantly across samples. If\n",
      "we observe a large p-value, for example larger than 0.05 or 0.1,\n",
      "then we cannot reject the null hypothesis of identical average scores.\n",
      "If the p-value is smaller than the threshold, e.g. 1%, 5% or 10%,\n",
      "then we reject the null hypothesis of equal averages.\n",
      "\n",
      "References\n",
      "----------\n",
      ".. [1] https://en.wikipedia.org/wiki/T-test#Independent_two-sample_t-test\n",
      "\n",
      ".. [2] https://en.wikipedia.org/wiki/Welch%27s_t-test\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from scipy import stats\n",
      ">>> np.random.seed(12345678)\n",
      "\n",
      "Test with sample with identical means:\n",
      "\n",
      ">>> rvs1 = stats.norm.rvs(loc=5,scale=10,size=500)\n",
      ">>> rvs2 = stats.norm.rvs(loc=5,scale=10,size=500)\n",
      ">>> stats.ttest_ind(rvs1,rvs2)\n",
      "(0.26833823296239279, 0.78849443369564776)\n",
      ">>> stats.ttest_ind(rvs1,rvs2, equal_var = False)\n",
      "(0.26833823296239279, 0.78849452749500748)\n",
      "\n",
      "`ttest_ind` underestimates p for unequal variances:\n",
      "\n",
      ">>> rvs3 = stats.norm.rvs(loc=5, scale=20, size=500)\n",
      ">>> stats.ttest_ind(rvs1, rvs3)\n",
      "(-0.46580283298287162, 0.64145827413436174)\n",
      ">>> stats.ttest_ind(rvs1, rvs3, equal_var = False)\n",
      "(-0.46580283298287162, 0.64149646246569292)\n",
      "\n",
      "When n1 != n2, the equal variance t-statistic is no longer equal to the\n",
      "unequal variance t-statistic:\n",
      "\n",
      ">>> rvs4 = stats.norm.rvs(loc=5, scale=20, size=100)\n",
      ">>> stats.ttest_ind(rvs1, rvs4)\n",
      "(-0.99882539442782481, 0.3182832709103896)\n",
      ">>> stats.ttest_ind(rvs1, rvs4, equal_var = False)\n",
      "(-0.69712570584654099, 0.48716927725402048)\n",
      "\n",
      "T-test with different means, variance, and n:\n",
      "\n",
      ">>> rvs5 = stats.norm.rvs(loc=8, scale=20, size=100)\n",
      ">>> stats.ttest_ind(rvs1, rvs5)\n",
      "(-1.4679669854490653, 0.14263895620529152)\n",
      ">>> stats.ttest_ind(rvs1, rvs5, equal_var = False)\n",
      "(-0.94365973617132992, 0.34744170334794122)\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/lib/python3.7/site-packages/scipy/stats/stats.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n"
     ],
     "name": "stdout"
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Read scipy's t-test implementation docs:\n",
    "ttest_ind?"
   ]
  },
  {
   "source": [
    "Now compute the t-statistics and obtain your p-value:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the t-test:\n",
    "results = ttest_ind()   # plug in your data\n",
    "\n",
    "# print output with descriptions:\n",
    "print(\"My t-statistic is \", results[0])\n",
    "print(\"My p-value is \", results[1])"
   ]
  },
  {
   "source": [
    "Should you be using the current output of a 2-tailed p-value for this experiment? Or should you be using a one-tailed p-value? What is your estimated one-tailed p-value? \n",
    "\n",
    " > Your answer here"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "If we are going by the popular stance of $p<0.05$ being significant, is your current t-test's result significant? What are the contributing factors to your p-value? \n",
    " > Your answer here"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "You think that there should be a treatment effect in your current data, so you googled around and found Cohen's D as a metric for effect size, the formula being:\n",
    "\n",
    "$$$\n",
    "d = \\frac{u_1 - u_2}{s}\n",
    "$$$\n",
    "\n",
    "Where $s$ is the pooled standard deviation for two independent samples:\n",
    "\n",
    "$$$\n",
    "s = \\sqrt{ \\frac{SD_1^2 + SD_2^2}{2}}\n",
    "$$$\n",
    "\n",
    "With $SD_1$ and $SD_2$ being standard deviations for the two samples respectively.\n",
    "\n",
    "So you set out to compute this metric with the following readily made function:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohen_d(a, b):\n",
    "    \"\"\"\n",
    "    Compute effect size with Cohen's D for samples a and b. \n",
    "    \"\"\"\n",
    "\t# calculate the means of the sample\n",
    "    u1 = np.mean(a)\n",
    "    u2 = np.mean(b)\n",
    "    # calculate the simplified pooled variance:\n",
    "    s = np.sqrt((np.std(a)**2 + np.std(b)**2)/2)\n",
    "    return (u1 - u2)/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-65-e25caaaa5c45>, line 4)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-65-e25caaaa5c45>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    print(\"My current effect size is \" d)\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Use the above function to compute your effect size:\n",
    "d = cohen_d()\n",
    "\n",
    "print(\"My current effect size is \" d)"
   ]
  },
  {
   "source": [
    "Is your current effect size large? Use the [Cohen's D Wikipedia page](https://en.wikipedia.org/wiki/Effect_size#Cohen's_d) to judge if you are not too familiar with the metric. What does your $p$-value and $d$ metric say about your current results? Are they reliable? \n",
    " \n",
    " > Your answer here"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Now repeat the above experiment with $N=40$ while everything else is the same:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 40      # New sample size\n",
    "\n",
    "# Generate your data:\n",
    "\n",
    "# Compute the T-statistic:\n",
    "\n",
    "# Compute the Cohen's D:\n",
    "\n",
    "# Print out your answers:"
   ]
  },
  {
   "source": [
    "You should re-execute the above code cell for multiple trials of your experiment, you should be seeing a lot of different results because the variance in your data. \n",
    "\n",
    "What conclusions can you draw about the relationship of sample size and $p$-value? Why should you report effect size in addition to $p$-values?\n",
    " \n",
    " > Your answer here"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## P-Hacking:\n",
    "\n",
    "P-hacking is where scientists try to achieve a significant result by increasing the sample size. In the following for-loop, we are dealing with data that's normally distributed with a small effect size (variance of 2, but difference in mean is only 1). \n",
    "\n",
    "Run this code with different sample sizes (N), and see how the output changes, don't be shy with N, you can go crazy high, but something as high as 500,000 might take a minute or two to run. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "\n",
    "count_significant = 0   # start with 0 count\n",
    "ntrials = 1000          # repeat the loop 1000 times\n",
    "for i in range(ntrials):\n",
    "    # Generate data\n",
    "    a = np.random.normal(1,2,N)\n",
    "    b = np.random.normal(0,2,N)\n",
    "\n",
    "    # Compute statistics\n",
    "    result = ttest_ind(a,b)\n",
    "\n",
    "    # If pvalue < 0.05, count it.\n",
    "    if result[1] < 0.05:\n",
    "        count_significant += 1\n",
    "\n",
    "print('The t-test was significant {}% of the time'.format((count_significant/ntrials)*100))"
   ]
  },
  {
   "source": [
    "The $p$-value is supposed to indicate your chances of getting a false positive result. At what sample size (N) did you start seeing significant results more than 5% of the time? 20%? 50%? ~90%? \n",
    "\n",
    " > Answer here"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}